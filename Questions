import numpy as np
import pandas as pd
import deepchem as dc
from deepchem.models.multitask import SingletaskToMultitask
from deepchem import metrics
from deepchem.metrics import Metric
from deepchem.models.sklearn_models import SklearnModel

import os
import sys
import deepchem as dc
from deepchem.molnet.load_function.molnet_loader import TransformerGenerator, _MolnetLoader
from deepchem.data import Dataset
from typing import List, Optional, Tuple, Union

from deepchem.molnet.load_function.qm7_datasets import load_qm7
tasks, datasets, transformers= dc.molnet.load_qm7(featurizer=dc.feat.CoulombMatrix(23))   #There is a bug.
'''
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-55-a76ad9107043> in <module>
----> 1 tasks, datasets, transformers= dc.molnet.load_qm7(featurizer=dc.feat.CoulombMatrix(23))

~\anaconda3\lib\site-packages\deepchem\molnet\load_function\qm7_datasets.py in load_qm7(featurizer, splitter, transformers, reload, data_dir, save_dir, **kwargs)
    105   loader = _QM7Loader(featurizer, splitter, transformers, QM7_TASKS, data_dir,
    106                       save_dir, **kwargs)
--> 107   return loader.load_dataset('qm7', reload)

~\anaconda3\lib\site-packages\deepchem\molnet\load_function\molnet_loader.py in load_dataset(self, name, reload)
    175 
    176     logger.info("About to featurize %s dataset." % name)
--> 177     dataset = self.create_dataset()
    178 
    179     # Split and transform the dataset.

~\anaconda3\lib\site-packages\deepchem\molnet\load_function\qm7_datasets.py in create_dataset(self)
     25     loader = dc.data.SDFLoader(
     26         tasks=self.tasks, featurizer=self.featurizer, sanitize=True)
---> 27     return loader.create_dataset(dataset_file, shard_size=8192)
     28 
     29 

~\anaconda3\lib\site-packages\deepchem\data\data_loader.py in create_dataset(self, inputs, data_dir, shard_size)
    833         yield X, y, w, ids
    834 
--> 835     return DiskDataset.create_dataset(shard_generator(), data_dir, self.tasks)
    836 
    837   def _get_shards(self, input_files: List[str],

~\anaconda3\lib\site-packages\deepchem\data\datasets.py in create_dataset(shard_generator, data_dir, tasks)
   1217     metadata_rows = []
   1218     time1 = time.time()
-> 1219     for shard_num, (X, y, w, ids) in enumerate(shard_generator):
   1220       basename = "shard-%d" % shard_num
   1221       metadata_rows.append(

~\anaconda3\lib\site-packages\deepchem\data\data_loader.py in shard_generator()
    811 
    812     def shard_generator():
--> 813       for shard_num, shard in enumerate(self._get_shards(inputs, shard_size)):
    814         time1 = time.time()
    815         X, valid_inds = self._featurize_shard(shard)

~\anaconda3\lib\site-packages\deepchem\utils\data_utils.py in load_sdf_files(input_files, clean_mols, tasks, shard_size)
    262       if not has_csv:  # Get task targets from .sdf file
    263         for task in tasks:
--> 264           df_row.append(mol.GetProp(str(task)))
    265       df_rows.append(df_row)
    266       if shard_size is not None and len(df_rows) == shard_size:

KeyError: 'u0_atom'
'''




tasks_sider, datasets_sider, transformers_sider = dc.molnet.load_sider(featurizer='ECFP') 
model_CNN_sider = dc.models.CNN(n_tasks=12, n_features=1024, dims=1, kernel_size=3, mode='regression')
model_CNN_sider.fit(train_dataset_sider, nb_epoch=10) # There is a bug

'''
WARNING:tensorflow:Model was constructed with shape (None, None, 1024) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1024), dtype=tf.float32, name='input_5'), name='input_5', description="created by layer 'input_5'"), but it was called on an input with incompatible shape (100, 1024, 1).
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-43-7d8cc9570bd8> in <module>
----> 1 model_CNN_sider.fit(train_dataset_sider, nb_epoch=10)

~\anaconda3\lib\site-packages\deepchem\models\keras_model.py in fit(self, dataset, nb_epoch, max_checkpoints_to_keep, checkpoint_interval, deterministic, restore, variables, loss, callbacks, all_losses)
    353     The average loss over the most recent checkpoint interval
    354    """
--> 355     return self.fit_generator(
    356         self.default_generator(
    357             dataset, epochs=nb_epoch,

~\anaconda3\lib\site-packages\deepchem\models\keras_model.py in fit_generator(self, generator, max_checkpoints_to_keep, checkpoint_interval, restore, variables, loss, callbacks, all_losses)
    442         inputs = inputs[0]
    443 
--> 444       batch_loss = apply_gradient_for_batch(inputs, labels, weights, loss)
    445       current_step = self._global_step.numpy()
    446 

~\anaconda3\lib\site-packages\tensorflow\python\util\traceback_utils.py in error_handler(*args, **kwargs)
    151     except Exception as e:
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153       raise e.with_traceback(filtered_tb) from None
    154     finally:
    155       del filtered_tb

~\anaconda3\lib\site-packages\deepchem\models\keras_model.py in tf__apply_gradient_for_batch(inputs, labels, weights, loss)
     11                 retval_ = ag__.UndefinedReturnValue()
     12                 with ag__.ld(tf).GradientTape() as tape:
---> 13                     outputs = ag__.converted_call(ag__.ld(self).model, (ag__.ld(inputs),), dict(training=True), fscope)
     14 
     15                     def get_state():

~\anaconda3\lib\site-packages\keras\utils\traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

~\anaconda3\lib\site-packages\keras\engine\input_spec.py in assert_input_compatibility(input_spec, inputs, layer_name)
    246           value = value.value
    247         if value is not None and shape_as_list[int(axis)] not in {value, None}:
--> 248           raise ValueError(
    249               f'Input {input_index} of layer "{layer_name}" is '
    250               f'incompatible with the layer: expected axis {axis} '

ValueError: in user code:

    File "C:\Users\ASUS\anaconda3\lib\site-packages\deepchem\models\keras_model.py", line 500, in apply_gradient_for_batch  *
        outputs = self.model(inputs, training=True)
    File "C:\Users\ASUS\anaconda3\lib\site-packages\keras\utils\traceback_utils.py", line 67, in error_handler  **
        raise e.with_traceback(filtered_tb) from None
    File "C:\Users\ASUS\anaconda3\lib\site-packages\keras\engine\input_spec.py", line 248, in assert_input_compatibility
        raise ValueError(

    ValueError: Exception encountered when calling layer "model_2" (type Functional).
    
    Input 0 of layer "conv1d_1" is incompatible with the layer: expected axis -1 of input shape to have value 1024, but received input with shape (100, 1024, 1)
    
    Call arguments received by layer "model_2" (type Functional):
      • inputs=['tf.Tensor(shape=(100, 1024, 1), dtype=float32)', 'tf.Tensor(shape=(1,), dtype=float32)']
      • training=True
      • mask=None
'''





model_GAT_sider = dc.models.GATModel(mode='classification', n_tasks=12,batch_size=16, learning_rate=0.001)
model_GAT_sider.fit(train_dataset_sider, nb_epoch=10) # There is a bug
'''
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-72-9cd987cfc561> in <module>
----> 1 model_GAT_sider.fit(train_dataset_sider, nb_epoch=10)

~\anaconda3\lib\site-packages\deepchem\models\torch_models\torch_model.py in fit(self, dataset, nb_epoch, max_checkpoints_to_keep, checkpoint_interval, deterministic, restore, variables, loss, callbacks, all_losses)
    332     The average loss over the most recent checkpoint interval
    333    """
--> 334     return self.fit_generator(
    335         self.default_generator(
    336             dataset, epochs=nb_epoch,

~\anaconda3\lib\site-packages\deepchem\models\torch_models\torch_model.py in fit_generator(self, generator, max_checkpoints_to_keep, checkpoint_interval, restore, variables, loss, callbacks, all_losses)
    414         restore = False
    415       inputs: OneOrMany[torch.Tensor]
--> 416       inputs, labels, weights = self._prepare_batch(batch)
    417 
    418       # Execute the loss function, accumulating the gradients.

~\anaconda3\lib\site-packages\deepchem\models\torch_models\gat.py in _prepare_batch(self, batch)
    363 
    364     inputs, labels, weights = batch
--> 365     dgl_graphs = [
    366         graph.to_dgl_graph(self_loop=self._self_loop) for graph in inputs[0]
    367     ]

~\anaconda3\lib\site-packages\deepchem\models\torch_models\gat.py in <listcomp>(.0)
    364     inputs, labels, weights = batch
    365     dgl_graphs = [
--> 366         graph.to_dgl_graph(self_loop=self._self_loop) for graph in inputs[0]
    367     ]
    368     inputs = dgl.batch(dgl_graphs).to(self.device)

AttributeError: 'numpy.ndarray' object has no attribute 'to_dgl_graph'
'''
'''
